"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"3.0.0","banner":null,"badge":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"docs":[{"type":"link","label":"Quick Start","href":"/docs/quick-start/","docId":"quick-start/quick-start"},{"type":"category","label":"Introduction","collapsed":false,"items":[{"type":"link","label":"Setting up","href":"/docs/introduction/setting-up","docId":"introduction/setting-up"},{"type":"link","label":"First crawler","href":"/docs/introduction/first-crawler","docId":"introduction/first-crawler"},{"type":"link","label":"CheerioCrawler","href":"/docs/introduction/cheerio-crawler","docId":"introduction/cheerio-crawler"},{"type":"link","label":"Enqueuing links","href":"/docs/introduction/enqueue-links","docId":"introduction/enqueue-links"},{"type":"link","label":"Realworld example","href":"/docs/introduction/realworld-example","docId":"introduction/realworld-example"}],"collapsible":true,"href":"/docs/introduction/"},{"type":"category","label":"Guides","items":[{"type":"link","label":"Motivation","href":"/docs/","docId":"guides/motivation"},{"type":"link","label":"Request Storage","href":"/docs/guides/request-storage","docId":"guides/request-storage"},{"type":"link","label":"Result Storage","href":"/docs/guides/result-storage","docId":"guides/result-storage"},{"type":"link","label":"Environment Variables","href":"/docs/guides/environment-variables","docId":"guides/environment-variables"},{"type":"link","label":"Proxy Management","href":"/docs/guides/proxy-management","docId":"guides/proxy-management"},{"type":"link","label":"Session Management","href":"/docs/guides/session-management","docId":"guides/session-management"},{"type":"link","label":"Scaling our crawlers","href":"/docs/guides/scaling-crawlers","docId":"guides/scaling-crawlers"},{"type":"link","label":"Avoid getting blocked","href":"/docs/guides/avoid-blocking","docId":"guides/avoid-blocking"},{"type":"link","label":"Got Scraping","href":"/docs/guides/got-scraping","docId":"guides/got-scraping"},{"type":"link","label":"TypeScript Projects","href":"/docs/guides/typescript-project","docId":"guides/typescript-project"},{"type":"link","label":"Running in Docker","href":"/docs/guides/docker-images","docId":"guides/docker-images"},{"type":"link","label":"Apify Platform","href":"/docs/guides/apify-platform","docId":"guides/apify-platform"}],"collapsed":true,"collapsible":true,"href":"/docs/guides"},{"type":"category","label":"Examples","items":[{"type":"link","label":"Accept user input","href":"/docs/examples/accept-user-input","docId":"examples/accept-user-input"},{"type":"link","label":"Add data to dataset","href":"/docs/examples/add-data-to-dataset","docId":"examples/add-data-to-dataset"},{"type":"link","label":"Basic crawler","href":"/docs/examples/basic-crawler","docId":"examples/basic-crawler"},{"type":"link","label":"Cheerio crawler","href":"/docs/examples/cheerio-crawler","docId":"examples/cheerio-crawler"},{"type":"link","label":"Crawl all links on a website","href":"/docs/examples/crawl-all-links","docId":"examples/crawl-all-links"},{"type":"link","label":"Crawl multiple URLs","href":"/docs/examples/crawl-multiple-urls","docId":"examples/crawl-multiple-urls"},{"type":"link","label":"Crawl a website with relative links","href":"/docs/examples/crawl-relative-links","docId":"examples/crawl-relative-links"},{"type":"link","label":"Crawl a single URL","href":"/docs/examples/crawl-single-url","docId":"examples/crawl-single-url"},{"type":"link","label":"Crawl a sitemap","href":"/docs/examples/crawl-sitemap","docId":"examples/crawl-sitemap"},{"type":"link","label":"Crawl some links on a website","href":"/docs/examples/crawl-some-links","docId":"examples/crawl-some-links"},{"type":"link","label":"Forms","href":"/docs/examples/forms","docId":"examples/forms"},{"type":"link","label":"Dataset Map and Reduce methods","href":"/docs/examples/map-and-reduce","docId":"examples/map-and-reduce"},{"type":"link","label":"Playwright crawler","href":"/docs/examples/playwright-crawler","docId":"examples/playwright-crawler"},{"type":"link","label":"Capture a screenshot using Puppeteer","href":"/docs/examples/capture-screenshot","docId":"examples/capture-screenshot"},{"type":"link","label":"Puppeteer crawler","href":"/docs/examples/puppeteer-crawler","docId":"examples/puppeteer-crawler"},{"type":"link","label":"Puppeteer recursive crawl","href":"/docs/examples/puppeteer-recursive-crawl","docId":"examples/puppeteer-recursive-crawl"},{"type":"link","label":"Puppeteer with proxy","href":"/docs/examples/puppeteer-with-proxy","docId":"examples/puppeteer-with-proxy"},{"type":"link","label":"Skipping navigations for certain requests","href":"/docs/examples/skip-navigation","docId":"examples/skip-navigation"}],"collapsed":true,"collapsible":true,"href":"/docs/examples"},{"type":"category","label":"Upgrading","items":[{"type":"link","label":"Upgrading to v1","href":"/docs/upgrading/upgrading-to-v1","docId":"upgrading/upgrading-to-v1"},{"type":"link","label":"Upgrading to v2","href":"/docs/upgrading/upgrading-to-v2","docId":"upgrading/upgrading-to-v2"},{"type":"link","label":"Upgrading to v3","href":"/docs/upgrading/upgrading-to-v3","docId":"upgrading/upgrading-to-v3"}],"collapsed":true,"collapsible":true,"href":"/docs/upgrading"}]},"docs":{"examples/accept-user-input":{"id":"examples/accept-user-input","title":"Accept user input","description":"This example accepts and logs user input:","sidebar":"docs"},"examples/add-data-to-dataset":{"id":"examples/add-data-to-dataset","title":"Add data to dataset","description":"This example saves data to the default dataset. If the dataset doesn\'t exist, it will be created.","sidebar":"docs"},"examples/basic-crawler":{"id":"examples/basic-crawler","title":"Basic crawler","description":"This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don\'t need to go this deep though, and it would be better to start with one of the full-featured crawlers","sidebar":"docs"},"examples/capture-screenshot":{"id":"examples/capture-screenshot","title":"Capture a screenshot using Puppeteer","description":"Using Puppeteer directly","sidebar":"docs"},"examples/cheerio-crawler":{"id":"examples/cheerio-crawler","title":"Cheerio crawler","description":"This example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags.","sidebar":"docs"},"examples/crawl-all-links":{"id":"examples/crawl-all-links","title":"Crawl all links on a website","description":"This example uses the enqueueLinks() method to add new links to the RequestQueue","sidebar":"docs"},"examples/crawl-multiple-urls":{"id":"examples/crawl-multiple-urls","title":"Crawl multiple URLs","description":"This example crawls the specified list of URLs.","sidebar":"docs"},"examples/crawl-relative-links":{"id":"examples/crawl-relative-links","title":"Crawl a website with relative links","description":"When crawling a website, you may encounter different types of links present that you may want to crawl.","sidebar":"docs"},"examples/crawl-single-url":{"id":"examples/crawl-single-url","title":"Crawl a single URL","description":"This example uses the got-scraping npm package","sidebar":"docs"},"examples/crawl-sitemap":{"id":"examples/crawl-sitemap","title":"Crawl a sitemap","description":"This example downloads and crawls the URLs from a sitemap, by using the downloadListOfUrls utility method provided by the @crawlee/utils module.","sidebar":"docs"},"examples/crawl-some-links":{"id":"examples/crawl-some-links","title":"Crawl some links on a website","description":"This CheerioCrawler example uses the globs property in the enqueueLinks() method to only add links to the RequestQueue queue if they match the specified pattern.","sidebar":"docs"},"examples/forms":{"id":"examples/forms","title":"Forms","description":"This example demonstrates how to use PuppeteerCrawler to","sidebar":"docs"},"examples/map-and-reduce":{"id":"examples/map-and-reduce","title":"Dataset Map and Reduce methods","description":"This example shows an easy use-case of the Dataset map","sidebar":"docs"},"examples/playwright-crawler":{"id":"examples/playwright-crawler","title":"Playwright crawler","description":"This example demonstrates how to use PlaywrightCrawler in combination with RequestQueue to recursively scrape the Hacker News website using headless Chrome / Playwright.","sidebar":"docs"},"examples/puppeteer-crawler":{"id":"examples/puppeteer-crawler","title":"Puppeteer crawler","description":"This example demonstrates how to use PuppeteerCrawler in combination","sidebar":"docs"},"examples/puppeteer-recursive-crawl":{"id":"examples/puppeteer-recursive-crawl","title":"Puppeteer recursive crawl","description":"Run the following example to perform a recursive crawl of a website using PuppeteerCrawler.","sidebar":"docs"},"examples/puppeteer-with-proxy":{"id":"examples/puppeteer-with-proxy","title":"Puppeteer with proxy","description":"FIXME: is this staying?","sidebar":"docs"},"examples/skip-navigation":{"id":"examples/skip-navigation","title":"Skipping navigations for certain requests","description":"While crawling a website, you may encounter certain resources you\'d like to save, but don\'t need the full power of a crawler to do so (like images delivered through a CDN).","sidebar":"docs"},"guides/apify-platform":{"id":"guides/apify-platform","title":"Apify Platform","description":"Apify platform - large-scale and high-performance web scraping","sidebar":"docs"},"guides/avoid-blocking":{"id":"guides/avoid-blocking","title":"Avoid getting blocked","description":"How to avoid getting blocked when scraping","sidebar":"docs"},"guides/docker-images":{"id":"guides/docker-images","title":"Running in Docker","description":"Example Docker images to run your crawlers","sidebar":"docs"},"guides/environment-variables":{"id":"guides/environment-variables","title":"Environment Variables","description":"Using environment variables to configure Crawlee\'s parameters","sidebar":"docs"},"guides/got-scraping":{"id":"guides/got-scraping","title":"Got Scraping","description":"Intro","sidebar":"docs"},"guides/motivation":{"id":"guides/motivation","title":"Motivation","description":"Spoiler: we built it so you can focus on awesome scrapers","sidebar":"docs"},"guides/proxy-management":{"id":"guides/proxy-management","title":"Proxy Management","description":"Using proxies to get around those annoying IP-blocks","sidebar":"docs"},"guides/request-storage":{"id":"guides/request-storage","title":"Request Storage","description":"How to store the requests your crawler will go through","sidebar":"docs"},"guides/result-storage":{"id":"guides/result-storage","title":"Result Storage","description":"Where are you going to store all of that juicy scraped data?!","sidebar":"docs"},"guides/scaling-crawlers":{"id":"guides/scaling-crawlers","title":"Scaling our crawlers","description":"To infinity and beyond! ...within limits","sidebar":"docs"},"guides/session-management":{"id":"guides/session-management","title":"Session Management","description":"How to manage your cookies, proxy IP rotations and more","sidebar":"docs"},"guides/typescript-project":{"id":"guides/typescript-project","title":"TypeScript Projects","description":"Stricter, safer, and better development experience","sidebar":"docs"},"introduction/cheerio-crawler":{"id":"introduction/cheerio-crawler","title":"CheerioCrawler aka jQuery crawler","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/enqueue-links":{"id":"introduction/enqueue-links","title":"Using Crawlee to enqueue links like a boss","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/first-crawler":{"id":"introduction/first-crawler","title":"First crawler","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/index":{"id":"introduction/index","title":"Introduction","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/realworld-example":{"id":"introduction/realworld-example","title":"Getting some real-world data","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/setting-up":{"id":"introduction/setting-up","title":"Setting up","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"quick-start/quick-start":{"id":"quick-start/quick-start","title":"Quick Start","description":"Quick and easy steps to get started using Crawlee today!","sidebar":"docs"},"readme/introduction":{"id":"readme/introduction","title":"Crawlee: The scalable web crawling and scraping library for JavaScript","description":"npm version"},"readme/overview":{"id":"readme/overview","title":"overview","description":"Overview"},"readme/support":{"id":"readme/support","title":"support","description":"Support"},"upgrading/upgrading-to-v1":{"id":"upgrading/upgrading-to-v1","title":"Upgrading to v1","description":"Summary","sidebar":"docs"},"upgrading/upgrading-to-v2":{"id":"upgrading/upgrading-to-v2","title":"Upgrading to v2","description":"- BREAKING: Require Node.js >=15.10.0 because HTTP2 support on lower Node.js versions is very buggy.","sidebar":"docs"},"upgrading/upgrading-to-v3":{"id":"upgrading/upgrading-to-v3","title":"Upgrading to v3","description":"This page summarizes most of the breaking changes between Crawlee (v3) and Apify SDK (v2). Crawlee is the spiritual successor to Apify SDK, so we decided to keep the versioning and release Crawlee as v3.","sidebar":"docs"}}}')}}]);
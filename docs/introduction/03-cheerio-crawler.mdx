---
id: cheerio-crawler
title: "CheerioCrawler aka jQuery crawler"
sidebar_label: "CheerioCrawler"
description: Your first steps into the world of scraping with Crawlee
---

import ApiLink from '@site/src/components/ApiLink';

This is the crawler that we used in our earlier example. Our simplest and also the fastest crawling solution. If you're familiar with `jQuery`, you'll understand <ApiLink to="cheerio-crawler/class/CheerioCrawler">CheerioCrawler</ApiLink> in minutes. [`Cheerio`](https://www.npmjs.com/package/cheerio) is essentially `jQuery` for Node.js. It offers the same API, including the familiar `$` object. You can use it, as you would use `jQuery`, for manipulating the DOM of an HTML page. In crawling, you'll mostly use it to select the right elements and extract their text values - the data you're interested in. But `jQuery` runs in a browser and attaches directly to the browser's DOM. Where does `cheerio` get its HTML? This is where the `Crawler` part of <ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> comes in.

### Overview

&#8203;<ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> crawls by making plain HTTP requests to the provided URLs. As you remember from the previous section, the URLs are fed to the crawler using either the <ApiLink to="core/class/RequestList">`RequestList`</ApiLink> or the <ApiLink to="core/class/RequestQueue">`RequestQueue`</ApiLink>. The HTTP responses it gets back are HTML pages, the same pages you would get in your browser when you first load a URL.

> Note, however, that modern web pages often do not serve all of their content in the first HTML response, but rather the first HTML contains links to other resources such as CSS and JavaScript that get downloaded afterwards, and together they create the final page. See <ApiLink to="puppeteer-crawler/class/PuppeteerCrawler">`PuppeteerCrawler`</ApiLink> and <ApiLink to="playwright-crawler/class/PlaywrightCrawler">`PlaywrightCrawler`</ApiLink> to crawl those.

Once the page's HTML is retrieved, the crawler will pass it to [Cheerio](https://www.npmjs.com/package/cheerio) for parsing. The result is the typical `$` function, which should be familiar to `jQuery` users. You can use this `$` to do all sorts of lookups and manipulation of the page's HTML, but in scraping, you will mostly use it to find specific HTML elements and extract their data.

Example use of Cheerio and its `$` function in comparison to browser JavaScript:

```ts
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio
```

> This is not to show that Cheerio is better than plain browser JavaScript. Some might actually prefer the more expressive way plain JS provides. Unfortunately, the browser JavaScript methods are not available in Node.js, so Cheerio is your best bet to do the parsing.

### When to use `CheerioCrawler`

Even though using `CheerioCrawler` is extremely easy, it probably will not be your first choice for most kinds of crawling or scraping in production environments. Since most websites nowadays use modern JavaScript to create rich, responsive and data-driven user experiences, the plain HTTP requests the crawler uses may just fall short of your needs.

But <ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> is far from useless! It really shines when you need to cope with extremely high workloads. With just 4 GBs of memory and a single CPU core, you can scrape 500 or more pages a minute! _(assuming each page contains approximately 400KB of HTML)_ To scrape this fast with a full browser scraper, such as the <ApiLink to="playwright-crawler/class/PlaywrightCrawler">`PlaywrightCrawler`</ApiLink>, you'd need significantly more computing power.

**Advantages:**

-   Extremely fast
-   Easy to set up
-   Familiar for jQuery users
-   Super cheap to run
-   Each request can go through a different proxy

**Disadvantages:**

-   Does not work for all websites
-   May easily overload the target website with requests
-   Does not enable any manipulation of the website before scraping

### Basic use of `CheerioCrawler`

Now that we have an idea of the crawler's inner workings, let's build one. We'll use the example from the previous section and improve on it by letting it truly crawl the page, finding new links as it goes, enqueuing them into the `RequestQueue` and then scraping them.

#### Refresher

Just to refresh your memory, in the previous section you built a very simple crawler that downloads the HTML of a single page, reads its title and prints
it to the console. This is the original source code:

```ts
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

// Enqueue the initial request and run the crawler
await crawler.run(['https://apify.com']);
```

Earlier we said that we would let the crawler:

1. Find new links on the page
2. Filter only those pointing to the same hostname, in this case `apify.com`
3. Enqueue them to the `RequestQueue`
4. Scrape the newly enqueued links

So let's get to it!

#### Finding new links

There are numerous approaches to finding links to follow when crawling the web. For our purposes, we will be looking for `<a>` elements that contain the `href` attribute. For example `<a href="https://apify.com/store">This is a link to Apify Store</a>`. To do this, we need to update our Cheerio function.

```ts
const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();
```

Our new function finds all the `<a>` elements that contain the `href` attribute and extracts the attributes into an array of strings. But there's a problem. There could be relative links in the list and those can't be used on their own. We need to resolve them using our domain as base URL and we will use one of Node.js' standard libraries to do this.

Crawlee exposes two URL properties: <ApiLink to="core/class/Request#url">`request.url`</ApiLink> and <ApiLink to="core/class/Request#loadedUrl">`request.loadedUrl`</ApiLink>. What's the difference? Well, `request.url` is the URL of the first request. In case of redirects, the URL changes. The final one is stored as `request.loadedUrl`.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const { hostname } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));
```

#### Filtering links to same domain

Websites typically contain a lot of links that lead away from the original page. This is normal, but when crawling a website, we usually want to crawl that one site and not let our crawler wander away to Google, Facebook and Twitter. Therefore, we need to filter out the off-domain links and only keep the ones that lead to the same domain.

> Don't worry, we'll soon learn about much easier way to handle this.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

const { hostname } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls.filter(url => url.hostname.endsWith(hostname));

// ...
```

This includes subdomains. In order to filter the same origin, simply compare the `url.origin` property instead:

```ts
const { origin } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls.filter(url => url.origin === origin);
```

> The `URL` class contains many other useful properties. You can read more about `url.origin` [here](https://developer.mozilla.org/en-US/docs/Web/API/URL/origin).

#### Enqueueing links to `RequestQueue`

This should be easy, because we already did that [earlier](#putting-it-all-together), remember? Just call `requestQueue.addRequests()` for all the new links. This will add them to the end of the queue for processing.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

const { origin } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls
    .filter(url => url.origin === origin)
    .map(url => ({ url: url.href }));

// Add the requests in series. There's of course room for speed
// improvement by parallelization. Try to implement it, if you wish.
console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
await requestQueue.addRequests(sameDomainLinks);

// ...
```

#### Scrape the newly enqueued links

And we're approaching the finishing line. All we need to do now is to integrate the new code into our original crawler. It will be easy, because almost everything needs to go into the `requestHandler`. But just before we do that, let's introduce the first crawler configuration option that is not a `requestHandler` or `requestQueue`. It's called `maxRequestsPerCrawl`.

##### The `maxRequestsPerCrawl` limit

This configuration option is available in all crawler classes, and you can use it to limit the number of `Request`s the crawler should process. It's very useful when you're just testing your code or when your crawler could potentially crawl millions of pages, and you want to save resources. You can add it to the crawler options like this:

```ts
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    requestHandler,
});
```

This limits the number of successfully handled `Request`s to 20. Bear in mind that the actual number of processed requests might be a little higher and that's because usually there are multiple `Request`s processed at the same time and once the 20th `Request` finishes, the other running `Request`s will be allowed to finish too.

#### Putting it all together

```ts
import { URL } from 'url'; // <------ This is new.
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20, // <------ This is new too.
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Here starts the new part of requestHandler.
        const links = $('a[href]')
            .map((i, el) => $(el).attr('href'))
            .get();

        const { origin } = new URL(request.loadedUrl);
        const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

        const sameDomainLinks = absoluteUrls
            .filter(url => url.origin === origin)
            .map(url => ({ url: url.href} ));

        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
        await crawler.addRequests(sameDomainLinks);
    },
});

await crawler.run(['https://apify.com']);
```

No matter if you followed along with your coding or just copy-pasted the resulting source, try running it now, perhaps even in both environments. You should see the crawler log the **title** of the first page, then the **enqueueing** message showing number of URLs, followed by the **title** of the first enqueued page and so on and so on.

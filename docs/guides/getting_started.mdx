---
id: getting-started
title: Getting Started
description: Your first steps into the world of scraping with Crawlee
---

import ApiLink from '@site/src/components/ApiLink';

Without the right tools, crawling and scraping the web can be difficult. At the very least, we need an HTTP client to make the necessary requests, but that only gets us raw HTML and sometimes not even that. Then we have to read this HTML and extract the data we're interested in. Once extracted, it must be stored in a machine-readable format and easily accessible for further processing, because it is the processed data that holds value.

Crawlee covers the process end-to-end. From crawling the web for links and scraping the raw data to storing it in various machine-readable formats, ready for processing. With this guide in hand, we should have our own data extraction solutions up and running in a few hours.

## Intro

The goal of this getting started guide is to provide a step-by-step introduction to all the features of Crawlee. It will walk us through creating the simplest of crawlers that only prints text to console, all the way up to complex systems that crawl pages, interact with them as if a real user were sitting in front of a real browser and output structured data.

## Setting up

To run Crawlee on our own computer, we need to meet the following pre-requisites first:

1. Have Node.js version 16.0 or higher installed.
 - Visit [Node.js website](https://nodejs.org/en/download/) to download or use [fnm](https://github.com/Schniz/fnm)
2. Have NPM installed.
 - NPM comes bundled with Node.js, so we should already have it. If not, reinstall Node.js.
 > Or use any other package manager of our choice.

If not certain, confirm the prerequisites by running:

```bash
node -v
```

```bash
npm -v
```

### Creating a new project

The fastest and best way to create new projects with Crawlee is to use the [Crawlee CLI](https://www.npmjs.com/package/@crawlee/cli). This command line tool allows us to create and run Crawlee projects with ease. We can use the `npx` utility to download and run the CLI - it is also embedded to the `crawlee` meta-package:

> We use `npx -y` to automatically allow installing the missing `crawlee` dependency in the global scope. It won't be necessary after we have the package downloaded.

```bash
npx -y crawlee create my-new-project
```

A prompt will be shown, asking us to choose a template. Let's choose the first one called `Crawlee playwright template [ts]`. The command will now create a new directory in our current working directory, called `my-new-project`, create a `package.json` in this folder and install all the necessary dependencies. It will also add example source code that we can immediately run.

Let's try that!

```bash
cd my-new-project
```

```bash
npx crawlee run
```

We used the `crawlee run` CLI command, but all what it actually does is to call `npm run start`. We can use the `--script` or `-s` option to specify what script we want to run instead of the default `start`:

```bash
npx crawlee run --script=start:prod
```

:::info Purging of storages

Another option of the `crawlee run` command is the control over purging: `--purge` and more importantly `--no-purge`, as purging is enabled by default. To understand what those are about, we first need to talk about the concept of storages in Crawlee. There are 3 types of a storage we can use: <ApiLink to="core/class/RequestQueue">`RequestQueue`</ApiLink>, <ApiLink to="core/class/KeyValueStore">`KeyValueStore`</ApiLink> and <ApiLink to="core/class/Dataset">`Dataset`</ApiLink>. Crawlee is using the first two for storing the runtime data about current crawl, and the third one is for storing the results.

When Crawlee stores some state, it uses a storage - by default a <ApiLink to="memory-storage/class/MemoryStorage">`MemoryStorage`</ApiLink>, which - contrary to its name - will also store the state in JSON files, so we can observe it easily. This state is being purged automatically when we run the crawler, and with the `--no-purge` flag we can disable this behaviour and reuse the state we already have.

More about storages and purging can be found in [Request Storage guide](./request-storage).

:::

We should start seeing log messages in the terminal as the system boots up and after a second, a Chromium browser window should pop up. In the window, we'll see quickly changing pages and back in the terminal, we should see the titles (contents of the `<title>` HTML tags) of the pages printed.

> We picked the playwright template, which will use the Chromium browser to open pages. If we picked the cheerio template instead, there won't be any browser window, as the requests to the target site will be done via [`got-scraping`](https://github.com/apify/got-scraping) instead of real browser.

We can always terminate the crawl with a keypress in the terminal:

```bash
CTRL+C
```

## First crawler

Now it's time to start writing some actual source code. But before we do, let's just briefly introduce all Crawlee classes necessary to make it happen.

### The general idea

There are 4 crawler classes available for use in Crawlee. <ApiLink to="basic-crawler/class/BasicCrawler">`BasicCrawler`</ApiLink>, <ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink>, <ApiLink to="puppeteer-crawler/class/PuppeteerCrawler">`PuppeteerCrawler`</ApiLink> and <ApiLink to="playwright-crawler/class/PlaywrightCrawler">`PlaywrightCrawler`</ApiLink>. We'll talk about their differences later. Now, let's talk about what they have in common.

The general idea of each crawler is to go to a web page, open it, do some stuff there, save some results and continue to the next page, until it's done its job. So the crawler always needs to find answers to two questions: **Where should I go?** and **What should I do there?** Answering those two questions is the only setup mandatory for running the crawlers.

### The Where - `Request`, `RequestList` and `RequestQueue`

All crawlers use instances of the <ApiLink to="core/class/Request">`Request`</ApiLink> class to determine where they need to go. Each request may hold a lot of information, but at the very least, it must hold a URL - a web page to open. But having only one URL would not make sense for crawling. We need to either have a pre-existing list of our own URLs that we wish to visit, perhaps a thousand, or a million, or we need to build this list dynamically as we crawl, adding more and more URLs to the list as we progress. Or both at the same time.

A representation of the pre-existing list is an instance of the <ApiLink to="core/class/RequestList">`RequestList`</ApiLink> class. It is a static, immutable list of URLs and other metadata (see the <ApiLink to="core/class/Request">`Request`</ApiLink> object) that the crawler will visit, one by one, retrying whenever an error occurs, until there are no more `Request`s to process.

&#8203;<ApiLink to="core/class/RequestQueue">`RequestQueue`</ApiLink> on the other hand, represents a dynamic queue of `Request`s. One that can be updated at runtime by adding more pages - `Request`s to process. This allows the crawler to open one page, extract interesting URLs, such as links to other pages on the same domain, add them to the queue (called _enqueuing_) and repeat this process to build a queue of tens of thousands or more URLs while knowing only a single one at the beginning.

`RequestList` and `RequestQueue` are essential for the crawler's operation. There is no other way to supply `Request`s = "pages to crawl" to the crawlers. At least one of them always needs to be provided while setting up. We can also use both at the same time, if we wish.

### The What - `requestHandler`

The `requestHandler` is the brain of the crawler. It tells it what to do at each and every page it visits. Generally it handles extraction of data from the page, processing the data, saving it, calling APIs, doing calculations and whatever else we need it to do.

The `requestHandler` is a user-defined function, invoked automatically by the crawler for each `Request` object, either from the `RequestList` or `RequestQueue`. It always receives a single argument - a plain object containing the crawling context. Its properties change depending on the crawler class used, but it always includes at least the `request` property, which represents the currently crawled `Request` instance (i.e. the URL the crawler is visiting and related metadata) and the `autoscaledPool` property, which is an instance of the <ApiLink to="core/class/AutoscaledPool">`AutoscaledPool`</ApiLink> class and we'll talk about it in detail later.

```ts
// The object received as a single argument by the requestHandler
{
    request: Request,
    autoscaledPool: AutoscaledPool
}
```

### Putting it all together

Enough theory! Let's put some of those hard-learned facts into practice. We learned above that we need `Request`s and a `requestHandler` to setup a crawler.

Let's start with something super easy. Visit a page, get its title and close. For the purposes of this tutorial, we'll be scraping our own webpage [https://apify.com](https://apify.com). Now, to get there, we need a `Request` with the page's URL in one of our sources, `RequestList` or `RequestQueue`. Let's go with `RequestQueue` for now.

```ts
import { RequestQueue } from 'crawlee';

// First we create the request queue instance.
const requestQueue = await RequestQueue.open();
// And then we add a request to it.
await requestQueue.addRequest({ url: 'https://apify.com' });
```

> If we're not familiar with the `async` and `await` keywords used in the example, we should know that these are native syntax in modern JavaScript. We can [learn more about them here](https://nikgrozev.com/2017/10/01/async-await/).

The <ApiLink to="core/class/RequestQueue#addRequest">`requestQueue.addRequest()`</ApiLink> function automatically converts the plain object we passed to it to a `Request` instance, so now we have a `requestQueue` that holds one `request` which points to `https://apify.com`. In case we want to add more requests, we can also use the <ApiLink to="core/class/RequestQueue#addRequests">`requestQueue.addRequests()`</ApiLink> method, that accepts an array of request like objects (or directly an array of strings - the URLs). Even better, there is a `crawler.addRequests()` method we will describe later in the tutorial - for now just keep in mind that the `crawler.addRequests()` is the smartest method of those three, because it supports adding large numbers of requests without blocking.

Now we need the `requestHandler`.

```ts
// Type import, applies only to TypeScript projects.
import type { CheerioCrawlingContext } from 'crawlee';

// We'll define the function separately, so it's more obvious.
const requestHandler: CheerioCrawlingContext = async ({ request, $ }) => {
    // This should look familiar if we ever worked with jQuery.
    // We're just getting the text content of the <title> HTML element.
    const title = $('title').text();

    console.log(`The title of "${request.url}" is: ${title}.`);
};
```

Wait, where did the `$` come from? Remember what we learned about the `requestHandler` earlier. It accepts a plain `Object` as an argument that will always have a `request` property, but it will also have other properties, depending on the chosen crawler class. Well, `$` is a property provided by the `CheerioCrawler` class, which we'll set up right now.

```ts
import { CheerioCrawler, RequestQueue, CheerioCrawlingContext } from 'crawlee';

const requestQueue = await RequestQueue.open();
await requestQueue.addRequests(['https://apify.com']);

const requestHandler: CheerioCrawlingContext = async ({ request, $ }) => {
    const title = $('title').text();
    console.log(`The title of "${request.url}" is: ${title}.`);
};

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    requestQueue,
    requestHandler,
});

await crawler.run();
```

That starts to look almost good. Almost? Yes, we can do even better! Every crawler has an implicit `RequestQueue` instance, so we can save ourselves from all the `RequestQueue` related code and just use `crawler.addRequests()` method. In fact, we can just use the first parameter of `crawler.run()`! We can also move the `requestHandler` method definition directly to the crawler options, this way we get type inference for free:

```ts
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

// Enqueue the initial request and run the crawler
await crawler.run(['https://apify.com']);
```

And we're done! We just created our first crawler from scratch. It will download the HTML of `https://apify.com`, find the `<title>` element, get its text content and print it to console. Good job!

We should see the message `The title of "https://apify.com" is: Web Scraping, Data Extraction and Automation · Apify.` printed to the screen. If we do, congratulations and let's move onto some bigger challenges! And if you feel like you don't really know what just happened there, no worries, it will all become clear when we learn more about `CheerioCrawler`.

:::info

We are using a feature called [Top level await](https://blog.saeloun.com/2021/11/25/ecmascript-top-level-await.html) in our examples. To be able to use that, we might need some extra setup. Namely, it requires the use of [ECMAScript Modules](https://nodejs.org/api/esm.html) - this means we either need to add `"type": "module"` to our `package.json` file, or use `*.mjs` extension for our files. Additionally, if we are in a TypeScript project, we need to set the `module` and `target` compiler options to `ES2022` or above.

:::

## CheerioCrawler aka jQuery crawler

This is the crawler that we used in our earlier example. Our simplest and also the fastest crawling solution. If we're familiar with `jQuery`, we'll understand <ApiLink to="cheerio-crawler/class/CheerioCrawler">CheerioCrawler</ApiLink> in minutes. [`Cheerio`](https://www.npmjs.com/package/cheerio) is essentially `jQuery` for Node.js. It offers the same API, including the familiar `$` object. We can use it, as we would use `jQuery`, for manipulating the DOM of an HTML page. In crawling, we'll mostly use it to select the right elements and extract their text values - the data we're interested in. But `jQuery` runs in a browser and attaches directly to the browser's DOM. Where does `cheerio` get its HTML? This is where the `Crawler` part of <ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> comes in.

### Overview

&#8203;<ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> crawls by making plain HTTP requests to the provided URLs. As we remember from the previous section, the URLs are fed to the crawler using either the <ApiLink to="core/class/RequestList">`RequestList`</ApiLink> or the <ApiLink to="core/class/RequestQueue">`RequestQueue`</ApiLink>. The HTTP responses it gets back are HTML pages, the same pages we would get in our browser when we first load a URL.

> Note, however, that modern web pages often do not serve all of their content in the first HTML response, but rather the first HTML contains links to other resources such as CSS and JavaScript that get downloaded afterwards, and together they create the final page. See <ApiLink to="puppeteer-crawler/class/PuppeteerCrawler">`PuppeteerCrawler`</ApiLink> and <ApiLink to="playwright-crawler/class/PlaywrightCrawler">`PlaywrightCrawler`</ApiLink> to crawl those.

Once the page's HTML is retrieved, the crawler will pass it to [Cheerio](https://www.npmjs.com/package/cheerio) for parsing. The result is the typical `$` function, which should be familiar to `jQuery` users. We can use this `$` to do all sorts of lookups and manipulation of the page's HTML, but in scraping, we will mostly use it to find specific HTML elements and extract their data.

Example use of Cheerio and its `$` function in comparison to browser JavaScript:

```ts
// Return the text content of the <title> element.
document.querySelector('title').textContent; // plain JS
$('title').text(); // Cheerio

// Return an array of all 'href' links on the page.
Array.from(document.querySelectorAll('[href]')).map(el => el.href); // plain JS
$('[href]')
    .map((i, el) => $(el).attr('href'))
    .get(); // Cheerio
```

> This is not to show that Cheerio is better than plain browser JavaScript. Some might actually prefer the more expressive way plain JS provides. Unfortunately, the browser JavaScript methods are not available in Node.js, so Cheerio is our best bet to do the parsing.

### When to use `CheerioCrawler`

Even though using `CheerioCrawler` is extremely easy, it probably will not be our first choice for most kinds of crawling or scraping in production environments. Since most websites nowadays use modern JavaScript to create rich, responsive and data-driven user experiences, the plain HTTP requests the crawler uses may just fall short of our needs.

But <ApiLink to="cheerio-crawler/class/CheerioCrawler">`CheerioCrawler`</ApiLink> is far from useless! It really shines when we need to cope with extremely high workloads. With just 4 GBs of memory and a single CPU core, we can scrape 500 or more pages a minute! _(assuming each page contains approximately 400KB of HTML)_ To scrape this fast with a full browser scraper, such as the <ApiLink to="playwright-crawler/class/PlaywrightCrawler">`PlaywrightCrawler`</ApiLink>, we'd need significantly more computing power.

**Advantages:**

-   Extremely fast
-   Easy to set up
-   Familiar for jQuery users
-   Super cheap to run
-   Each request can go through a different proxy

**Disadvantages:**

-   Does not work for all websites
-   May easily overload the target website with requests
-   Does not enable any manipulation of the website before scraping

### Basic use of `CheerioCrawler`

Now that we have an idea of the crawler's inner workings, let's build one. We'll use the example from the previous section and improve on it by letting it truly crawl the page, finding new links as it goes, enqueuing them into the `RequestQueue` and then scraping them.

#### Refresher

Just to refresh our memory, in the previous section we built a very simple crawler that downloads the HTML of a single page, reads its title and prints
it to the console. This is the original source code:

```ts
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);
    },
});

// Enqueue the initial request and run the crawler
await crawler.run(['https://apify.com']);
```

Earlier we said that we would let the crawler:

1. Find new links on the page
2. Filter only those pointing to the same hostname, in this case `apify.com`
3. Enqueue them to the `RequestQueue`
4. Scrape the newly enqueued links

So let's get to it!

#### Finding new links

There are numerous approaches to finding links to follow when crawling the web. For our purposes, we will be looking for `<a>` elements that contain the `href` attribute. For example `<a href="https://apify.com/store">This is a link to Apify Store</a>`. To do this, we need to update our Cheerio function.

```ts
const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();
```

Our new function finds all the `<a>` elements that contain the `href` attribute and extracts the attributes into an array of strings. But there's a problem. There could be relative links in the list and those can't be used on their own. We need to resolve them using our domain as base URL and we will use one of Node.js' standard libraries to do this.

Crawlee exposes two URL properties: <ApiLink to="core/class/Request#url">`request.url`</ApiLink> and <ApiLink to="core/class/Request#loadedUrl">`request.loadedUrl`</ApiLink>. What's the difference? Well, `request.url` is the URL of the first request. In case of redirects, the URL changes. The final one is stored as `request.loadedUrl`.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const { hostname } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));
```

#### Filtering links to same domain

Websites typically contain a lot of links that lead away from the original page. This is normal, but when crawling a website, we usually want to crawl that one site and not let our crawler wander away to Google, Facebook and Twitter. Therefore, we need to filter out the off-domain links and only keep the ones that lead to the same domain.

> Don't worry, we'll soon learn about much easier way to handle this.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

const { hostname } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls.filter(url => url.hostname.endsWith(hostname));

// ...
```

This includes subdomains. In order to filter the same origin, simply compare the `url.origin` property instead:

```ts
const { origin } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls.filter(url => url.origin === origin);
```

> The `URL` class contains many other useful properties. We can read more about `url.origin` [here](https://developer.mozilla.org/en-US/docs/Web/API/URL/origin).

#### Enqueueing links to `RequestQueue`

This should be easy, because we already did that [earlier](#putting-it-all-together), remember? Just call `requestQueue.addRequests()` for all the new links. This will add them to the end of the queue for processing.

```ts
// At the top of the file:
import { URL } from 'url';

// ...

const links = $('a[href]')
    .map((i, el) => $(el).attr('href'))
    .get();

const { origin } = new URL(request.loadedUrl);
const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

const sameDomainLinks = absoluteUrls
    .filter(url => url.origin === origin)
    .map(url => ({ url: url.href }));

// Add the requests in series. There's of course room for speed
// improvement by parallelization. Try to implement it, if we wish.
console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
await requestQueue.addRequests(sameDomainLinks);

// ...
```

#### Scrape the newly enqueued links

And we're approaching the finishing line. All we need to do now is to integrate the new code into our original crawler. It will be easy, because almost everything needs to go into the `requestHandler`. But just before we do that, let's introduce the first crawler configuration option that is not a `requestHandler` or `requestQueue`. It's called `maxRequestsPerCrawl`.

##### The `maxRequestsPerCrawl` limit

This configuration option is available in all crawler classes, and we can use it to limit the number of `Request`s the crawler should process. It's very useful when we're just testing our code or when our crawler could potentially crawl millions of pages, and we want to save resources. We can add it to the crawler options like this:

```ts
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    requestHandler,
});
```

This limits the number of successfully handled `Request`s to 20. Bear in mind that the actual number of processed requests might be a little higher and that's because usually there are multiple `Request`s processed at the same time and once the 20th `Request` finishes, the other running `Request`s will be allowed to finish too.

#### Putting it all together

```ts
import { URL } from 'url'; // <------ This is new.
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20, // <------ This is new too.
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Here starts the new part of requestHandler.
        const links = $('a[href]')
            .map((i, el) => $(el).attr('href'))
            .get();

        const { origin } = new URL(request.loadedUrl);
        const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

        const sameDomainLinks = absoluteUrls
            .filter(url => url.origin === origin)
            .map(url => ({ url: url.href} ));

        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
        await crawler.addRequests(sameDomainLinks);
    },
});

await crawler.run(['https://apify.com']);
```

No matter if we followed along with our coding or just copy-pasted the resulting source, try running it now, perhaps even in both environments. We should see the crawler log the **title** of the first page, then the **enqueueing** message showing number of URLs, followed by the **title** of the first enqueued page and so on and so on.

## Using Crawlee to enqueue links like a boss

If you were paying attention carefully in the previous chapter, we said there is much easier way to enqueue new `Request`s with a single function call. You might be wondering why you had to go through the whole process of getting the individual links, filtering the same domain ones and then manually enqueuing them into the `RequestQueue`, when there is a simpler way.

Well, the obvious reason is practice. This is a tutorial after all. The other reason is to make you think about all the bits and pieces that come together, so that in the end, a new page, not previously entered in by us, can be scraped. We think that by seeing the bigger picture, you will be able to get the most out of Crawlee.

### Introduction to `enqueueLinks()`

Since enqueuing new links to crawl is such an integral part of web crawling, we created a function that attempts to simplify this process as much as possible. With a single function call, it allows us to find all the links on a page that match specified criteria and add them to a `RequestQueue`. It also allows us to modify the resulting `Request`s to match our crawling needs.

`enqueueLinks` is quite a powerful function so, like crawlers, it gets its arguments from an options object. This is useful, because we don't have to remember their order! But also because we can easily extend its API and add new features. We can <ApiLink to="core/function/enqueueLinks">find the full reference here</ApiLink>.

```ts
import { enqueueLinks } from 'crawlee';

// Now we can use enqueueLinks like this:
await enqueueLinks({
    /* options */
});
```

### Basic use of `enqueueLinks()` with `CheerioCrawler`

We already implemented logic that takes care of enqueueing new links to a `RequestQueue` in the previous chapter on `CheerioCrawler`. Let's look at that logic and implement the same functionality using `enqueueLinks()`.

We found that the crawler needed to do these 4 things to crawl `apify.com`:

1. Find new links on the page
2. Filter only those pointing to `apify.com`
3. Enqueue them to the `RequestQueue`
4. Scrape the newly enqueued links

Using `enqueueLinks()` we can squash the first 3 into a single function call. We will use the context bound `enqueueLinks` variant:

```ts
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    async requestHandler({ enqueueLinks }) {
        await enqueueLinks();
    },
});

await crawler.run(['https://apify.com']);
```

Wait, what? Yes! That's all needed to find and enqueue all the links on currently handled page. We are using the context bound variant (the one from the `requestHandler`'s first context parameter), so it already knows what the current request body looks like, how to extract the links from it, how to handle relative links, it uses the crawler's `RequestQueue` (and creates a default one if we don't provide our own). By default, it will enqueue only links that target the same hostname (via the so called <ApiLink to="core/enum/EnqueueStrategy#SameHostname">`EnqueueStrategy.SameHostname`</ApiLink> strategy).

#### Using `enqueueLinks()` to filter links

While the defaults for `enqueueLinks` can be often exactly what we need, it also gives us fine-grained control over _what_ links should be enqueued. One way we already mentioned above is by using the <ApiLink to="core/enum/EnqueueStrategy">`EnqueueStrategy`</ApiLink>. We can use the `EnqueueStrategy.All` strategy if we want to follow every single link, regardless of its domain, or we can enqueue links that target the same domain name with the `EnqueueStrategy.SameDomain` strategy.

```ts
await enqueueLinks({
    strategy: EnqueueStrategy.SameDomain, // or 'same-domain'
});
````

For even more control, we can use `globs`, `regexps` and `pseudoUrls` to filter the URLs. Those arguments are always `Array`s, but their contents can take on many forms. <ApiLink to="core/interface/EnqueueLinksOptions">See the reference</ApiLink> for more information about them as well as other options.

> If we provide one of those options, the default `SameHostname` strategy will **not** be applied unless explicitly set in the options.

```ts
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
});
```

To have absolute control, we have the <ApiLink to="core/interface/EnqueueLinksOptions/#transformRequestFunction">`transformRequestFunction`</ApiLink>. Just before a new <ApiLink to="core/class/Request">`Request`</ApiLink> is constructed and enqueued to the <ApiLink to="core/class/RequestQueue">`RequestQueue`</ApiLink>, this function can be used to skip it or modify its contents such as `userData`, `payload` or, most importantly `uniqueKey`. This is useful when you need to enqueue multiple `Requests` to the queue that share the same URL, but differ in methods or payloads, or to dynamically update or create `userData`.

```ts
await enqueueLinks({
    globs: ['http?(s)://apify.com/*/*'],
    transformRequestFunction(req) {
        // ignore all links ending with `.html`
        if (req.url.endsWith('.html')) return false;
        return req;
    },
});
```

#### Integrating `enqueueLinks()` into our crawler

Now let's get back to our crawler. Let's take a look at the original crawler code, where we enqueued all the links manually.

```ts
import { URL } from 'url';
import { CheerioCrawler } from 'crawlee';

// Set up the crawler, passing a single options object as an argument.
const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    async requestHandler({ request, $ }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        // Here starts the new part of requestHandler.
        const links = $('a[href]')
            .map((i, el) => $(el).attr('href'))
            .get();

        const { origin } = new URL(request.loadedUrl);
        const absoluteUrls = links.map(link => new URL(link, request.loadedUrl));

        const sameDomainLinks = absoluteUrls
            .filter(url => url.origin === origin)
            .map(url => ({ url: url.href} ));

        console.log(`Enqueueing ${sameDomainLinks.length} URLs.`);
        await crawler.addRequests(sameDomainLinks);
    },
});

await crawler.run(['https://apify.com']);
```

Since we've already learned that the context bound `enqueueLinks()` function does exactly what we need, we can just replace all the above enqueuing logic with a single function call, as promised.

```ts
import { CheerioCrawler } from 'crawlee';

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 20,
    async requestHandler({ request, $, enqueueLinks }) {
        const title = $('title').text();
        console.log(`The title of "${request.url}" is: ${title}.`);

        const enqueued = await enqueueLinks();
        console.log(`Enqueued ${enqueued.length} URLs.`);
    },
});

await crawler.run(['https://apify.com']);
```

And that's it! No more parsing the links from HTML using Cheerio, filtering them and enqueueing them one by one It all gets done automatically! `enqueueLinks()` is just one example of Crawlee's powerful helper functions. They're all designed to make our life easier, so we can focus on getting our data, while leaving the mundane crawling management to the tools.

`enqueueLinks()` has a lot more tricks up its sleeve. Make sure to check out the <ApiLink to="core/interface/EnqueueLinksOptions">options reference</ApiLink> to see what else it can do for us. Namely the feature to prepopulate the `Request` instances it creates with `userData` of our choice is extremely useful!

## Getting some real-world data

> Hey, guys, you know, it's cool that we can scrape the `<title>` elements of web pages, but that's not very useful. Can we finally scrape some real data and save it somewhere in a machine-readable format? Because that's why we started reading this tutorial in the first place!

We hear you, young padawan! First, learn how to crawl, we must. Only then, walk through data, we can!

### Making a store crawler

Fortunately, we don't have to travel to a galaxy far far away to find a good candidate for learning how to scrape structured data. The [Apify Store](https://apify.com/store) is a library of public actors that anyone can grab and use. We can find ready-made solutions for crawling [Google Places](https://apify.com/drobnikj/crawler-google-places), [Amazon](https://apify.com/vaclavrut/amazon-crawler), [Google Search](https://apify.com/apify/google-search-scraper), [Booking](https://apify.com/dtrungtin/booking-scraper), [Instagram](https://apify.com/jaroslavhejlek/instagram-scraper), [Tripadvisor](https://apify.com/maxcopell/tripadvisor) and many other websites. Feel free to check them out! It's also a great place to practice our Jedi scraping skills since it has categories, lists and details. That's almost like our imaginary `online-store.com` from the previous chapter.

### The importance of having a plan

Sometimes scraping is really straightforward, but most of the times, it really pays to do a little bit of research first. How is the website structured? Can I scrape it only with HTTP requests (read "with `CheerioCrawler`") or would I need a full browser solution? Are there any anti-scraping protections in place? Do I need to parse the HTML or can I get the data otherwise, such as directly from the website's API. Jakub, one of Apify's founders, wrote a [great article about all the different techniques](https://blog.apify.com/web-scraping-in-2018-forget-html-use-xhrs-metadata-or-javascript-variables-8167f252439c) and tips and tricks, so make sure to check that out!

For the purposes of this tutorial, let's just go ahead with HTTP requests and HTML parsing using `CheerioCrawler`. The number one reason being: We already know how to use it, and we want to build on that knowledge to learn specific crawling and scraping techniques.

#### Choosing the data we need

A good first step is always to figure out what it is we want to scrape and where to find it. For the time being, let's just agree that we want to scrape all actors (see the `Show` dropdown) in all categories (which can be found on the left side of the page) and for each actor we want to get its

1.  URL
2.  Owner
3.  Unique identifier (such as `apify/web-scraper`)
4.  Title
5.  Description
6.  Last modification date
7.  Number of runs

We can see that some information is available directly on the list page, but for details such as "Last modification date" or "Number of runs" we'll also need to open the actor detail pages.

![data to scrape](/img/getting-started/scraping-practice.jpg 'Overview of data to be scraped.')

#### Analyzing the target

Knowing that we will use plain HTTP requests, we immediately know that we won't be able to manipulate the website in any way. We will only be able to go through the HTML it gives us and parse our data from there. This might sound like a huge limitation, but you might be surprised in how effective it can be. Let's get to it!

#### The start URL(s)

This is where we start our crawl. It's convenient to start as close to our data as possible. For example, it wouldn't make much sense to start at `apify.com` and look for a `store` link there, when we already know that everything we want to extract can be found at the `apify.com/store` page.

Once we look at the `apify.com/store` page more carefully, we see that the categories themselves produce URLs that we can use to access those individual categories.

```
https://apify.com/store?category=ENTERTAINMENT
```

Should we write down all the category URLs down and use all of them as start URLs? It's definitely possible, but what if a new category appears on the page later? We would not learn about it unless we manually visit the page and inspect it again. So scraping the category links off the store page certainly makes sense. This way we always get an up-to-date list of categories.

But is it really that straightforward? By digging further into the store page's HTML we find that it does not actually contain the category links. The menu on the left uses JavaScript to display the items from a given category and, as we've learned earlier, `CheerioCrawler` cannot execute JavaScript.

> We've deliberately chosen this scenario to show an example of the number one weakness of `CheerioCrawler`. We will overcome this difficulty in our `PlaywrightCrawler` tutorial, but at the cost of compute resources and speed. Always remember that no tool is best for everything!

So we're back to the pre-selected list of URLs. Since we cannot scrape the list dynamically, we have to manually collect the links and then use them in our crawler. We lose the ability to scrape new categories, but we keep the low resource consumption and speed advantages of `CheerioCrawler`.

Therefore, after careful consideration, we've determined that we should use multiple start URLs and that they should look as follows:

```
https://apify.com/store?category=TRAVEL
https://apify.com/store?category=ECOMMERCE
https://apify.com/store?category=ENTERTAINMENT
```

### The crawling strategy

Now that we know where to start, we need to figure out where to go next. Since we've eliminated one level of crawling by selecting the categories manually, we now only need to crawl the actor detail pages. The algorithm therefore follows:

1. Visit the category list page (one of our start URLs).
2. Enqueue all links to actor details.
3. Visit all actor details and extract data.
4. Repeat 1 - 3 for all categories.

> Technically, this is a depth first crawl and the crawler will perform a breadth first crawl by default, but that's an implementation detail. We've chosen this notation since a breadth first crawl would be less readable.

`CheerioCrawler` will make sure to visit the pages for us, if we provide the correct `Request`s and we already know how to enqueue pages, so this should be fairly easy. Nevertheless, there are two more tricks that we'd like to showcase.

#### Using a `RequestList`

`RequestList` is a perfect tool for scraping a pre-existing list of URLs and if we think about our start URLs, this is exactly what we have! A list of links to the different categories of the store. Let's see how we'd get them into a `RequestList`.

```ts
const sources = [
    'https://apify.com/store?category=TRAVEL',
    'https://apify.com/store?category=ECOMMERCE',
    'https://apify.com/store?category=ENTERTAINMENT',
];

const requestList = await RequestList.open('categories', sources);
```

As we can see, similarly to the `RequestQueue.open()` function, there is a `RequestList.open()` function that will create a `RequestList` instance for us. The first argument is the name of the `RequestList` (or an options object). It is used to persist the crawling state of the list. This is useful when we want to continue where we left off after an error or a process restart. The second argument is the `sources` array, which is nothing more than a list of URLs we wish to crawl.

> `RequestQueue` is a persistent store by default, so no name is needed, while the `RequestList` only lives in memory and giving it a name enables it to become persistent.

We might now want to ask one of these questions:

-   Can I enqueue into `RequestList` too?
-   How do I make `RequestList` work together with `RequestQueue` since I need the queue to enqueue new `Request`s.

The answer to the first one is a definitive no. `RequestList` is immutable and once we create it, we cannot add or remove `Request`s from it. The answer to the second one is simple. `RequestList` and `RequestQueue` are made to work together out-of-the-box in crawlers, so all we need to do is use them both and the crawlers will do the rest.

```ts
const crawler = new CheerioCrawler({
    requestList,
    requestQueue,
    requestHandler,
});
```

> For those wondering how this works, the `RequestList` `Request`s are enqueued into the `RequestQueue` right before their execution and only processed by the `RequestQueue` afterwards.

:::info Prefer `crawler.addRequests()`

While `RequestList` might be a good fit for some use cases like handling very large input lists, or when we don't need any dynamic enqueuing at all. Using `crawler.addRequests()` should be generally the preferred way to handle enqueuing of initial requests. It will enqueue the initial batch of 1000 requests and resolve right after that, so it won't block the start of crawling. Instead, it will continue adding more requests to the queue in batches in the background while the crawler runs.

Following section is here mainly to show how to work with `RequestList`.

:::

#### Sanity check

It's always useful to create some simple boilerplate code to see that we've got everything set up correctly before we start to write the scraping logic itself. We might realize that something in our previous analysis doesn't quite add up, or the website might not behave exactly as we expected.

Let's use our newly acquired `RequestList` knowledge and everything we know from the previous chapters to create a new crawler that'll just visit all the category URLs we selected and print the text content of all the actors in the category. Try running the code below in our selected environment. We should see, albeit very badly formatted, the text of the individual actor cards that are displayed in the selected categories.

```ts
import { CheerioCrawler, RequestList } from 'crawlee';

const sources = [
    'https://apify.com/store?category=TRAVEL',
    'https://apify.com/store?category=ECOMMERCE',
    'https://apify.com/store?category=ENTERTAINMENT',
];

const requestList = await RequestList.open('categories', sources);

const crawler = new CheerioCrawler({
    requestList,
    requestHandler: async ({ $ }) => {
        // Select all the actor cards.
        $('.item').each((i, el) => {
            const text = $(el).text();
            console.log(`ITEM: ${text}\n`);
        });
    },
});

await crawler.run();
```

We might be wondering how we got that `.item` selector. After analyzing the category pages using a browser's DevTools, we've determined that it's a good selector to select all the currently displayed actor cards. DevTools and CSS selectors are quite a large topic, so we can't go into too much detail now, but here are a few general pointers.

#### DevTools crash course

> We'll use Chrome DevTools here, since it's the most common browser, but feel free to use any other, it's all very similar.

We could pick any category, but let's just go with Travel because it includes some interesting actors. Let's open `https://apify.com/store?category=TRAVEL` in Chrome and open DevTools either by right-clicking anywhere in the page and selecting `Inspect`, or by pressing `F12` or by any other means relevant to our system. Once we're there, we'll see a bunch of DevToolsy stuff and a view of the category page with the individual actor cards.

Now, let's find the `Select an element` tool and use it to select one of the actor cards. We need to make sure to select the whole card, not just some of its contents, such as its title or description.

In the resulting HTML display, it will put our cursor somewhere. Inspecting the HTML around it, we'll see that there are some CSS classes attached to the different HTML elements.

By hovering over the individual elements, we will see their placement in the page's view. It's easy to see the page's structure around the actor cards now. All the cards are displayed in a `<div>` with a classname that starts with `ItemsGrid__StyledDiv`, which holds another `<div>` with some computer-generated class names and finally, inside this `<div>`, the individual cards are represented by other `<div>` elements with the class of `item`.

> Yes, there are other HTML elements and other classes too. We can safely ignore them.

It should now make sense how we got that `.item` selector. It's just a selector that finds all elements that are annotated with the `item` class and those just happen to be the actor cards only.

It's always a good idea to double-check that though, so go into the DevTools Console and run

```ts
document.querySelectorAll('.item');
```

We will see that only the actor cards will be returned, and nothing else.

#### Enqueueing the detail links using a custom selector

In the previous chapter, we used the `enqueueLinks()` function like this:

```ts
await enqueueLinks();
```

While very useful in that scenario, we need something different now. Instead of finding all the `<a href="..">` targeting the same hostname, we need to find only the specific ones that will take us to the actor detail pages. Otherwise, we'd be visiting a lot of other pages that we're not interested in. Using the power of DevTools and yet another `enqueueLinks()` parameter, this becomes fairly easy.

```ts
import { CheerioCrawlingContext } from 'crawlee';

export async function requestHandler({ request, enqueueLinks }: CheerioCrawlingContext) {
    console.log(`Processing ${request.url}`);

    // Only enqueue new links from the category pages.
    if (!request.userData.detailPage) {
        await enqueueLinks({
            selector: 'div.item > a',
            userData: { detailPage: true },
        });
    }
}
```

The code should look pretty familiar to us. It's a very simple `requestHandler` where we log the currently processed URL to the console and enqueue more links. But there are also a few new, interesting additions. Let's break it down.

##### The `selector` parameter of `enqueueLinks()`

When we previously used `enqueueLinks()`, we were not providing any `selector` parameter, and it was fine, because we wanted to use the default value, which is `a` - finds all `<a>` elements. But now, we need to be more specific. There are multiple `<a>` links on the given category page, and we're only interested in those that will take us to item (actor) details. Using the DevTools, we found out that we can select the links we wanted using the `div.item > a` selector, which selects all the `<a>` elements that have a `<div class="item ...">` parent. And those are exactly the ones we're interested in.

##### Finally, the `userData` of `enqueueLinks()`

We will see `userData` used often throughout Crawlee, and it's nothing more than a place to store our own data on a `Request` instance. We can access it with `request.userData` and it's a plain `Object` that can be used to store anything that needs to survive the full life-cycle of the `Request`. We can also use `Request.label` shortcut, that under the hood controls the `userData.label`.

To modify all the `Request` instances before enqueueing, we can use the `transformRequestFunction` option of `enqueueLinks()`. In our case, we use it to set a `userData.detailPage` property to the enqueued `Request`s to be able to easily differentiate between the category pages and the detail pages.

#### Another sanity check

It's always good to work step by step. We have this new enqueueing logic in place and since the previous [Sanity check](#sanity-check) worked only with a `RequestList`, because we were not enqueueing anything, don't forget to add back the `RequestQueue` and `maxRequestsPerCrawl` limit. Let's test it out!

```ts
import { CheerioCrawler, RequestList, RequestQueue } from 'crawlee';

const sources = [
    'https://apify.com/store?category=TRAVEL',
    'https://apify.com/store?category=ECOMMERCE',
    'https://apify.com/store?category=ENTERTAINMENT',
];

const requestList = await RequestList.open('categories', sources);
const requestQueue = await RequestQueue.open(); // <----------------

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 50, // <----------------------------------------
    requestList,
    requestQueue, // <---------------------------------------------------
    async requestHandler({ request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // Only enqueue new links from the category pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div.item > a',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

We've added the `requestHandler()` with the `enqueueLinks()` logic from the previous section to the code we wrote earlier. As always, try running it in the environment of our choice. We should see the crawler output a number of links to the console, as it crawls the category pages first and then all the links to the actor detail pages it found.

This concludes our Crawling strategy section, because we have taught the crawler to visit all the pages we need. Let's continue with scraping the tasty data.

### Scraping data

At the beginning of this chapter, we created a list of the information we wanted to collect about the actors in the store. Let's review that and figure out ways to access it.

1. URL
2. Owner
3. Unique identifier (such as `apify/web-scraper`)
4. Title
5. Description
6. Last modification date
7. Number of runs

![data to scrape](/img/getting-started/scraping-practice.jpg 'Overview of data to be scraped.')

#### Scraping the URL, Owner and Unique identifier

Some information is lying right there in front of us without even having to touch the actor detail pages. The `URL` we already have - the `request.url`. And by looking at it carefully, we realize that it already includes the `owner` and the `unique identifier` too. We can just split the `string` and be on our way then!

> We could also use the `request.loadedUrl` instead. Remember the difference: `request.url` is what you enqueue, `request.loadedUrl` is what gets processed (after possible redirects).

```ts
// request.url = https://apify.com/apify/web-scraper

const urlArr = request.url.split('/').slice(-2); // ['apify', 'web-scraper']
const uniqueIdentifier = urlArr.join('/'); // 'apify/web-scraper'
const owner = urlArr[0]; // 'apify'
```

> It's always a matter of preference, whether to store this information separately in the resulting dataset, or not. Whoever uses the dataset can easily parse the `owner` from the `URL`, so should we duplicate the data unnecessarily? Our opinion is that unless the increased data consumption would be too large to bear, it's always better to make the dataset as readable as possible. Someone might want to filter by `owner` for example and keeping only the `URL` in the dataset would make this complicated without using additional tools.

#### Scraping Title, Description, Last modification date and Number of runs

Now it's time to add more data to the results. Let's open one of the actor detail pages in the Store, for example the [`apify/web-scraper`](https://apify.com/apify/web-scraper) page and use our DevTools-Fu to figure out how to get the title of the actor.

##### Title

![actor title](/img/getting-started/title-01.jpg 'Finding actor title in DevTools.')

By using the element selector tool, we find out that the title is there under an `<h1>` tag, as titles should be. Maybe surprisingly, we find that there are actually two `<h1>` tags on the detail page. This should get us thinking. Is there any parent element that includes our `<h1>` tag, but not the other ones? Yes, there is! There is a `<header>` element that we can use to select only the heading we're interested in.

> Remember that we can press CTRL+F (or CMD+F on Mac) in the Elements tab of DevTools to open the search bar where we can quickly search for elements using their selectors. And always make sure to use the DevTools to verify our scraping process and assumptions. It's faster than changing the crawler code all the time.

To get the title we just need to find it using `Cheerio` and a `header h1` selector, which selects all `<h1>` elements that have a `<header>` ancestor. And as we already know, there's only one.

```ts
return {
    title: $('header h1').text(),
};
```

##### Description

Getting the actor's description is a little more involved, but still pretty straightforward. We can't just simply search for a `<p>` tag, because there's a lot of them in the page. We need to narrow our search down a little. Using the DevTools we find that the actor description is nested within the `<header>` element too, same as the title. Moreover, the actual description is nested inside a `<span>` tag with a class `actor-description`.

![actor description selector](/img/getting-started/description.jpg 'Finding actor description in DevTools.')

```ts
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
};
```

##### Last modification date

The DevTools tell us that the `modifiedDate` can be found in the `<time>` element inside `<ul class="ActorHeader-stats">`.

![actor last modification date selector](/img/getting-started/modified-date.jpg 'Finding actor last modification date in DevTools.')

```ts
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
        // or we can do just `+$('ul.ActorHeader-stats time').attr('datetime')`
    ),
};
```

It might look a little too complex at first glance, but let's walk through it. We find the right `<time>` element, and then we read its `datetime` attribute, because that's where a unix timestamp is stored as a `string`.

But we would much rather see a readable date in our results, not a unix timestamp, so we need to convert it. Unfortunately the `new Date()` constructor will not accept a `string`, so we cast the `string` to a `number` using the `Number()` function before actually calling `new Date()`. Phew!

##### Run count

And so we're finishing up with the `runCount`. There's no specific element like `<time>`, so we need to create a complex selector and then do a transformation on the result.

```ts
return {
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
    ),
    runCount: Number(
        $('ul.ActorHeader-stats > li:nth-of-type(3)')
            .text()
            .match(/[\d,]+/)[0]
            .replace(',', ''),
    ),
};
```

The `ul.ActorHeader-stats > li:nth-of-type(3)` looks complicated, but it only reads that we're looking for a `<ul class="ActorHeader-stats ...">` element and within that element we're looking for the third `<li>` element. We grab its text, but we're only interested in the number of runs. So we parse the number out using a regular expression, but its type is still a `string`, so we finally convert the result to a `number` by wrapping it with a `Number()` call.

> The numbers are formatted with commas as thousands separators (e.g. `'1,234,567'`), so to extract it, we first use regular expression `/[\d,]+/` - it will search for consecutive number or comma characters. Then we extract the match via `.match(/[\d,]+/)[0]` and finally remove the commas by calling `.replace(',', '')`. This will give us a string (e.g. `'1234567'`) that can be converted via `Number` function.

And there we have it! All the data we needed in a single object. For the sake of completeness, let's add the properties we parsed from the URL earlier, and we're good to go.

```ts
const urlArr = request.url.split('/').slice(-2);

const results = {
    url: request.url,
    uniqueIdentifier: urlArr.join('/'),
    owner: urlArr[0],
    title: $('header h1').text(),
    description: $('header span.actor-description').text(),
    modifiedDate: new Date(
        Number(
            $('ul.ActorHeader-stats time').attr('datetime'),
        ),
    ),
    runCount: Number(
        $('ul.ActorHeader-stats > li:nth-of-type(3)')
            .text()
            .match(/[\d,]+/)[0]
            .replace(',', ''),
    ),
};

console.log('RESULTS: ', results);
```

#### Trying it out (sanity check #3)

We have everything we need so just grab our newly created scraping logic, dump it into our original `requestHandler()` and see the magic happen!

```ts
import { CheerioCrawler, RequestList, RequestQueue } from 'crawlee';

const sources = [
    'https://apify.com/store?category=TRAVEL',
    'https://apify.com/store?category=ECOMMERCE',
    'https://apify.com/store?category=ENTERTAINMENT',
];

const requestList = await RequestList.open('categories', sources);
const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 50,
    requestList,
    requestQueue,
    async requestHandler({ $, request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // This is our new scraping logic.
        if (request.userData.detailPage) {
            const urlArr = request.url.split('/').slice(-2);

            const results = {
                url: request.url,
                uniqueIdentifier: urlArr.join('/'),
                owner: urlArr[0],
                title: $('header h1').text(),
                description: $('header span.actor-description').text(),
                modifiedDate: new Date(
                    Number(
                        $('ul.ActorHeader-stats time').attr('datetime'),
                    ),
                ),
                runCount: Number(
                    $('ul.ActorHeader-stats > li:nth-of-type(3)')
                        .text()
                        .match(/[\d,]+/)[0]
                        .replace(',', ''),
                ),
            };
            console.log('RESULTS', results);
        }

        // Only enqueue new links from the category pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div.item > a',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

> Notice again that we're scraping on the detail pages `request.userData.detailPage === true`, but we're only enqueueing on the category pages `request.userData.detailPage === undefined`.

When running the actor in the environment of our choice, we should see the crawled URLs and their scraped data printed to the console.

### Saving the scraped data

A data extraction job would not be complete without saving the data for later use and processing. We've come to the final and most difficult part of this chapter so make sure to pay attention very carefully!

First, replace the `console.log('RESULTS', results)` call with

```ts
import { Dataset } from 'crawlee';

await Dataset.pushData(results);
```

and that's it. Unlike in the previous paragraph, we are being serious now. That's it, we're done. The final code therefore looks exactly like this:

```ts
import { CheerioCrawler, RequestList, RequestQueue, Dataset } from 'crawlee';

const sources = [
    'https://apify.com/store?category=TRAVEL',
    'https://apify.com/store?category=ECOMMERCE',
    'https://apify.com/store?category=ENTERTAINMENT',
];

const requestList = await RequestList.open('categories', sources);
const requestQueue = await RequestQueue.open();

const crawler = new CheerioCrawler({
    maxRequestsPerCrawl: 50,
    requestList,
    requestQueue,
    async requestHandler({ $, request, enqueueLinks }) {
        console.log(`Processing ${request.url}`);

        // This is our new scraping logic.
        if (request.userData.detailPage) {
            const urlArr = request.url.split('/').slice(-2);

            const results = {
                url: request.url,
                uniqueIdentifier: urlArr.join('/'),
                owner: urlArr[0],
                title: $('header h1').text(),
                description: $('header span.actor-description').text(),
                modifiedDate: new Date(
                    Number(
                        $('ul.ActorHeader-stats time').attr('datetime'),
                    ),
                ),
                runCount: Number(
                    $('ul.ActorHeader-stats > li:nth-of-type(3)')
                        .text()
                        .match(/[\d,]+/)[0]
                        .replace(',', ''),
                ),
            };
            await Dataset.pushData(results);
        }

        // Only enqueue new links from the category pages.
        if (!request.userData.detailPage) {
            await enqueueLinks({
                selector: 'div.item > a',
                userData: { detailPage: true },
            });
        }
    },
});

await crawler.run();
```

#### What's `Dataset.pushData()`

&#8203;<ApiLink to="core/class/Dataset#pushData">`Dataset.pushData()`</ApiLink> is a helper function that saves data to the default <ApiLink to="core/class/Dataset">`Dataset`</ApiLink>. `Dataset` is a storage designed to hold virtually unlimited amount of data in a format similar to a table. Each time we call `Dataset.pushData()` a new row in the table is created, with the property names serving as column titles.

> Each crawler run has one default `Dataset` so no need to initialize it or create an instance first. It just gets done automatically for us. We can also create named datasets at will.

#### Finding our saved data

It might not be perfectly obvious where the data we saved using the previous command went. Unless we changed the environment variables that Crawlee uses locally, which would suggest that we knew what we were doing, and we didn't need this tutorial anyway, we'll find our data in the `crawlee_storage` directory:

```
{PROJECT_FOLDER}/crawlee_storage/datasets/default/
```

The above folder will hold all our saved data in numbered files, as they were pushed into the dataset. Each file represents one invocation of `Dataset.pushData()` or one table row.

> In case you were running the crawler on the Apify Platform, you can find the results in the Dataset tab on the Run page.

### Final touch

It may seem that the data are extracted and the crawler is done, but honestly, this is just the beginning. For the sake of brevity, we've completely omitted error handling, proxies, debug logging, tests, documentation and other stuff that a reliable software should have. The good thing is, **error handling is mostly done by Crawlee itself**, so no worries on that front, unless we need some custom magic.

Anyway, to spark some ideas, let's look at two more things. First, passing an input to the crawler, which will enable us to change the categories we want to scrape without changing the source code itself! And then some refactoring, to show how we reckon is preferable to structure and annotate crawler code.

#### Meet the `INPUT`

`INPUT` is just a convention on how we call the crawler's input. Because there's no magic in crawlers, just features, the `INPUT` is actually nothing more
than a key in the default <ApiLink to="core/class/KeyValueStore">`KeyValueStore`</ApiLink> that's, by convention, used as input on the Apify platform. Also by convention, the
`INPUT` is mostly expected to be of `Content-Type: application/json`.

We will not go into `KeyValueStore` details here, but for the sake of `INPUT` we need to remember that there is a function that helps we get it.

```ts
const input = await KeyValueStore.getInput();
```

> On the Apify Platform, the crawler's input that we can set in the Console is automatically saved to the default `KeyValueStore` under the key `INPUT` and by calling <ApiLink to="core/class/KeyValueStore#getInput">`KeyValueStore.getInput()`</ApiLink> we retrieve the value from the `KeyValueStore`.

Running locally, we need to place an `INPUT.json` file in our default key value store for this to work.

```
{PROJECT_FOLDER}/crawlee_storage/key_value_stores/default/INPUT.json
```

#### Use `INPUT` to seed our crawler with categories

Currently, we're using the full URLs of categories as sources, but it's quite obvious that we only need the final parameters, the rest of the URL is always the same. Knowing that, we can pass an array of those parameters on `INPUT` and build the URLs dynamically, which would allow us to scrape different categories without changing the source code. Let's get to it!

For simplicity, let's set the default input in the code directly, as a fallback in case the input is not found in the default `KeyValueStore`:

> We are using the [nullish coalescing operator (`??`)](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Nullish_coalescing_operator), it will use the value on the right hand side only if the left hand side resolves to `null` or `undefined`.

```ts
const input = await KeyValueStore.getInput() ?? ['TRAVEL', 'ECOMMERCE', 'ENTERTAINMENT'];
```

Next, we will be using the categories in the input to construct the category URLs, and we'll also pass custom `userData` to the sources. This means that the `Request`s that get created will automatically contain this `userData`.

```ts
// ...
const input = await KeyValueStore.getInput() ?? ['TRAVEL', 'ECOMMERCE', 'ENTERTAINMENT'];

const sources = input.map(category => ({
    url: `https://apify.com/store?category=${category}`,
    userData: {
        label: 'CATEGORY',
    },
}));

const requestList = await RequestList.open('categories', sources);
// ...
```

The `userData.label` is also a convention that we've been using for quite some time to label different `Request`s. We know that this is a category URL so we `label` it `CATEGORY`. This way, we can easily make decisions in the `requestHandler` without having to inspect the URL itself.

We can then refactor the `if` clauses in the `requestHandler` to use the `label` for decision-making. This does not make much sense for a crawler with only two different pages, because a simple `boolean` would suffice, but for pages with multiple different views, it becomes very useful.

#### Structuring the code better

But perhaps we should not stop at refactoring the `if` clauses. There are several ways we can make the crawler look more elegant and - most importantly - easier to reason about and make changes to.

In the following code we've made several changes.

[//]: # (TODO continue here, namely adjust the text based on new code sample)

-   Split the code into multiple files.
-   Replaced `console.log` with the crawlee logger.
-   Added a `getSources()` function to encapsulate `INPUT` consumption.
-   Added a `createRouter()` function to make our routing cleaner, without nested `if` clauses.
-   Removed the `maxRequestsPerCrawl` limit.

In our `main.js` file, we place the general structure of the crawler:

```ts title="main.js"
import { CheerioCrawler, log } from 'crawlee';
import { router } from './routes.js';
import { getSources } from './tools.js';

log.info('Starting crawler.');

log.debug('Setting up crawler.');
const crawler = new CheerioCrawler({
    requestHandler: router,
});

await crawler.addRequests(await getSources());

log.info('Starting the crawl.');
await crawler.run();
log.info('Actor finished.');
```

Then in a separate `tools.js`, we add our helper functions:

```ts title="tools.js"
import { KeyValueStore, log } from 'crawlee';

export const DEFAULT_CATEGORIES = ['TRAVEL', 'ECOMMERCE', 'ENTERTAINMENT'];

export async function getSources() {
    log.debug('Getting sources.');
    const input = await KeyValueStore.getInput() ?? DEFAULT_CATEGORIES;
    return input.map(category => ({
        url: `https://apify.com/store?category=${category}`,
        label: 'CATEGORY',
    }));
}
```

And finally our routes in a separate `routes.js` file:

```ts title="routes.js"
import { createCheerioRouter, Dataset } from 'crawlee';

export const router = createCheerioRouter();

router.addHandler('CATEGORY', async ({ enqueueLinks }) => {
    await enqueueLinks({
        selector: 'div.item > a',
        label: 'DETAIL',
    });
});

router.addHandler('DETAIL', async ({ $, log, request }) => {
    const urlArr = request.url.split('/').slice(-2);

    log.debug('Scraping results.');
    const results = {
        url: request.url,
        uniqueIdentifier: urlArr.join('/'),
        owner: urlArr[0],
        title: $('header h1').text(),
        description: $('header span.actor-description').text(),
        modifiedDate: new Date(
            Number(
                $('ul.ActorHeader-stats time').attr('datetime'),
            ),
        ),
        runCount: Number(
            $('ul.ActorHeader-stats > li:nth-of-type(3)')
                .text()
                .match(/[\d,]+/)[0]
                .replace(',', ''),
        ),
    };

    log.debug('Pushing data to dataset.');
    await Dataset.pushData(results);
});
```

Let's describe the changes a bit more in detail. We hope that in the end, we can agree that this structure makes the actor more readable and manageable.

#### Splitting our code into multiple files

This was not always the case, but now that the Apify platform has a multifile editor, there's no reason not to split our code into multiple files and keep
our logic separate. Less code in a single file means less code we need to think about at any time, and that's a great thing!

#### Using Crawlee `log` instead of `console.log`

We won't go to great lengths here to talk about `log` object from Crawlee, because we can read <ApiLink to="core/class/Log">it all in the documentation</ApiLink>, but there's just one thing that we need to stress: **log levels**.

`utils.log` enables us to use different log levels, such as `log.debug`, `log.info` or `log.warning`. It not only makes our log more readable, but it also allows selective turning off of some levels by either calling the `log.setLevel()` function or by setting an `APIFY_LOG_LEVEL` variable. This is huge! Because we can now add a lot of debug logs in our actor, which will help we when something goes wrong and turn them on or off with a simple `INPUT` change, or by setting an environment variable.

The punch line? Use `log` exported from `crawlee` instead of `console.log` now and thank us later when something goes wrong!

#### Using a router to structure our crawling

At first, it might seem more readable using just a simple `if / else` statement to select different logic based on the crawled pages, but trust me, it
becomes far less impressive when working with more than two different pages, and it definitely starts to fall apart when the logic to handle each page
spans tens or hundreds of lines of code.

It's good practice in any programming to split our logic into bite-sized chunks that are easy to read and reason about. Scrolling through a
thousand line long `requestHandler()` where everything interacts with everything and variables can be used everywhere is not a beautiful thing to
do and a pain to debug. That's why we prefer the separation of routes into a special file and with large routes, we would even suggest having one file
per route.

## Running the crawler in Cloud

Now that we have our crawler ready, it's the right time to think about where we want to run it. We should already have docker image ready for the crawler, as it was generated by the CLI. To read more about how to run this docker image in the cloud, check out the [Apify Platform guide](./apify-platform).

[//]: # (> TO BE CONTINUED with details on `PlaywrightCrawler` and other features...)

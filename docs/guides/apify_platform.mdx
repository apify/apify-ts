---
id: apify-platform
title: Apify Platform
---

import ApiLink from '@site/src/components/ApiLink';

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

import MainSource from '!!raw-loader!./apify_platform_main.ts';
import InitExitSource from '!!raw-loader!./apify_platform_init_exit.ts';

Apify is a [platform](https://apify.com) built to serve large-scale and high-performance web scraping
and automation needs. It provides easy access to [compute instances (Actors)](#what-is-an-actor),
convenient [request](../guides/request-storage) and [result](../guides/result-storage) storages, [proxies](../guides/proxy-management),
[scheduling](https://docs.apify.com/scheduler), [webhooks](https://docs.apify.com/webhooks)
and [more](https://docs.apify.com/), accessible through a [web interface](https://console.apify.com)
or an [API](https://docs.apify.com/api).

While we think that the Apify platform is super cool, and you should definitely sign up for a
[free account](https://console.apify.com/sign-up), **Crawlee is and will always be open source**,
runnable locally or on any cloud infrastructure.

:::note

We do not test Crawlee in other cloud environments such as Lambda or on specific
architectures such as Raspberry PI. We strive to make it work, but there are no guarantees.

:::

## Logging into Apify platform from Crawlee

To access your [Apify account](https://console.apify.com/sign-up) from Crawlee, you must provide
credentials - [your API token](https://console.apify.com/account#/integrations). You can do that
either by utilizing [Apify CLI](https://github.com/apify/apify-cli) or with environment
variables.

Once you provide credentials to your scraper, you will be able to use all the Apify platform
features, such as calling actors, saving to cloud storages, using Apify proxies,
setting up webhooks and so on.

### Log in with CLI

Apify CLI allows you to log in to your Apify account on your computer. If you then run your
scraper using the CLI, your credentials will automatically be added.

```bash
npm install -g apify-cli
apify login -t YOUR_API_TOKEN
```

### Log in with environment variables

If you prefer not to use Apify CLI, you can always provide credentials to your scraper
by setting the [`APIFY_TOKEN`](../guides/environment-variables#apify_token) environment
variable to your API token.

> There's also the [`APIFY_PROXY_PASSWORD`](../guides/environment-variables#apify_proxy_password)
> environment variable. Crawlee automatically infers that from your token, but it can be useful
> when you need to access proxies from a different account than your token represents.

## What is an actor

When you deploy your script to the Apify platform, it becomes an [actor](https://apify.com/actors).
An actor is a serverless microservice that accepts an input and produces an output. It can run for
a few seconds, hours or even infinitely. An actor can perform anything from a simple action such
as filling out a web form or sending an email, to complex operations such as crawling an entire website
and removing duplicates from a large dataset.

Actors can be shared in the [Apify Store](https://apify.com/store) so that other people can use them.
But don't worry, if you share your actor in the store and somebody uses it, it runs under their account,
not yours.

**Related links**

- [Store of existing actors](https://apify.com/store)
- [Documentation](https://docs.apify.com/actors)
- [View actors in Apify Console](https://console.apify.com/actors)
- [API reference](https://apify.com/docs/api/v2#/reference/actors)

## Running an actor locally

First let's create a boilerplate of the new actor. We could use Apify CLI and just run:

```bash
apify create my-hello-world
```

The CLI will prompt us to select a project boilerplate template - let's pick "Hello world". The tool will create a directory called `my-hello-world` with a Node.js project files. We can run the actor as follows:

```bash
cd my-hello-world
apify run
```

## Running Crawlee code as an actor

For running the Crawlee code as an actor on the [Apify platform](https://apify.com/actors) we should either:
- wrap it into <ApiLink to="apify/class/Actor#main">`Actor.main()`</ApiLink> function;
- or use a combination of <ApiLink to="apify/class/Actor#init">`Actor.init()`</ApiLink> and <ApiLink to="apify/class/Actor#exit">`Actor.exit()`</ApiLink> functions.

Let's look at the `CheerioCrawler` example from the [Quick Start](../guides/quick-start) guide:

<Tabs>
    <TabItem value="main" label="Using Actor.main()" default>
        <CodeBlock language="js">
            {MainSource}
        </CodeBlock>
    </TabItem>
    <TabItem value="init_exit" label="Using Actor.init() and Actor.exit()">
        <CodeBlock language="js">
            {InitExitSource}
        </CodeBlock>
    </TabItem>
</Tabs>

Note that we could also run our actor (that is using Crawlee) locally with Apify CLI. We could start it via the following command in our project folder:

```bash
apify run -p
```

## Deploying an actor to Apify platform

Now (assuming we are already logged in to our Apify account) we can easily deploy our code to the Apify platform by running:

```bash
apify push
```

Our script will be uploaded to the Apify platform and built there so that it can be run. For more information, view the
[Apify Actor](https://docs.apify.com/cli) documentation.

## Usage on Apify platform

We can also develop our actor in an online code editor directly on the platform (we'll need an Apify Account). Let's go to the [Actors](https://console.apify.com/actors) page in the app, click *Create new* and then go to the *Source* tab and start writing our code or paste one of the examples from the Examples section.

## Storages

There are several things worth mentioning here.

1. Compared to Crawlee, in order to simplify access to the default <ApiLink to="core/class/KeyValueStore">`Key-Value Store`</ApiLink> and <ApiLink to="core/class/Dataset">`Dataset`</ApiLink> we don't need to use the helper functions of storage classes, but instead, we could use <ApiLink to="apify/class/Actor#getValue">`Actor.getValue()`</ApiLink>, <ApiLink to="apify/class/Actor#setValue">`Actor.setValue()`</ApiLink> for the default `Key-Value Store` and <ApiLink to="apify/class/Actor#pushData">`Actor.pushData()`</ApiLink> for the default `Dataset` directly.
2. In order to open the storage, we shouldn't use the storage classes, but instead use the Actor class. Thus, instead of <ApiLink to="core/class/KeyValueStore#open">`KeyValueStore.open()`</ApiLink>, <ApiLink to="core/class/Dataset#open">`Dataset.open()`</ApiLink> and <ApiLink to="core/class/RequestQueue#open">`RequestQueue.open()`</ApiLink>, we could use <ApiLink to="apify/class/Actor#openKeyValueStore">`Actor.openKeyValueStore()`</ApiLink>, <ApiLink to="apify/class/Actor#openDataset">`Actor.openDataset()`</ApiLink> and <ApiLink to="apify/class/Actor#openRequestQueue">`Actor.openRequestQueue()`</ApiLink> respectively. Using each of these methods allows us to pass the <ApiLink to="apify/interface/OpenStorageOptions">`OpenStorageOptions`</ApiLink> as a second argument, which has only one optional property: <ApiLink to="apify/interface/OpenStorageOptions#forceCloud">`forceCloud`</ApiLink>. If set to `true` - cloud storage will be used instead of the folder on the local disk.
3. When the <ApiLink to="core/class/Dataset">`Dataset`</ApiLink> is stored on the `Apify platform`, we can export its data to the following formats: HTML, JSON, CSV, Excel, XML and RSS. The datasets are displayed on the actor run details page and in the [Storage](https://console.apify.com/storage) section in the Apify Console. The actual data is exported using the [Get dataset items](https://apify.com/docs/api/v2#/reference/datasets/item-collection/get-items) Apify API endpoint. This way you can easily share the crawling results.

**Related links**

- [Apify platform storage documentation](https://docs.apify.com/storage)
- [View storage in Apify Console](https://console.apify.com/storage)
- [Key-value stores API reference](https://apify.com/docs/api/v2#/reference/key-value-stores)
- [Datasets API reference](https://docs.apify.com/api/v2#/reference/datasets)
- [Request queues API reference](https://docs.apify.com/api/v2#/reference/request-queues)

---
id: apify-platform
title: Apify Platform
description: Apify platform - large-scale and high-performance web scraping
---

import ApiLink from '@site/src/components/ApiLink';

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import CodeBlock from '@theme/CodeBlock';

import MainSource from '!!raw-loader!./apify_platform_main.ts';
import InitExitSource from '!!raw-loader!./apify_platform_init_exit.ts';

Apify is a [platform](https://apify.com) built to serve large-scale and high-performance web scraping
and automation needs. It provides easy access to [compute instances (Actors)](#what-is-an-actor),
convenient [request](../guides/request-storage) and [result](../guides/result-storage) storages, [proxies](../guides/proxy-management),
[scheduling](https://docs.apify.com/scheduler), [webhooks](https://docs.apify.com/webhooks)
and [more](https://docs.apify.com/), accessible through a [web interface](https://console.apify.com)
or an [API](https://docs.apify.com/api).

While we think that the Apify platform is super cool, and it's definitely worth signing up for a
[free account](https://console.apify.com/sign-up), **Crawlee is and will always be open source**,
runnable locally or on any cloud infrastructure.

:::note

We do not test Crawlee in other cloud environments such as Lambda or on specific
architectures such as Raspberry PI. We strive to make it work, but there are no guarantees.

:::

## Logging into Apify platform from Crawlee

To access our [Apify account](https://console.apify.com/sign-up) from Crawlee, we must provide
credentials - [our API token](https://console.apify.com/account?tab=integrations). We can do that
either by utilizing [Apify CLI](https://github.com/apify/apify-cli) or with environment
variables.

Once we provide credentials to our scraper, we will be able to use all the Apify platform
features, such as calling actors, saving to cloud storages, using Apify proxies,
setting up webhooks and so on.

### Log in with CLI

Apify CLI allows us to log in to our Apify account on our computer. If we then run our
scraper using the CLI, our credentials will automatically be added.

```bash
npm install -g apify-cli
apify login -t OUR_API_TOKEN
```

### Log in with environment variables

Alternatively, we can always provide credentials to our scraper
by setting the [`APIFY_TOKEN`](#apify_token) environment
variable to our API token.

> There's also the [`APIFY_PROXY_PASSWORD`](#apify_proxy_password)
> environment variable. Actor automatically infers that from our token, but it can be useful
> when we need to access proxies from a different account than our token represents.

### Log in with Configuration

Another option is to use the [`Configuration`](https://apify.github.io/apify-sdk-js/api/apify/class/Configuration) instance and set our api token there.

```javascript
import { Actor } from 'apify';

const sdk = new Actor({ token: 'our_api_token' });
```

## What is an actor

When we deploy our script to the Apify platform, it becomes an [actor](https://apify.com/actors).
An actor is a serverless microservice that accepts an input and produces an output. It can run for
a few seconds, hours or even infinitely. An actor can perform anything from a simple action such
as filling out a web form or sending an email, to complex operations such as crawling an entire website
and removing duplicates from a large dataset.

Actors can be shared in the [Apify Store](https://apify.com/store) so that other people can use them.
But don't worry, if we share our actor in the store and somebody uses it, it runs under their account,
not ours.

**Related links**

- [Store of existing actors](https://apify.com/store)
- [Documentation](https://docs.apify.com/actors)
- [View actors in Apify Console](https://console.apify.com/actors)
- [API reference](https://apify.com/docs/api/v2#/reference/actors)

## Running an actor locally

First let's create a boilerplate of the new actor. We could use Apify CLI and just run:

```bash
apify create my-hello-world
```

The CLI will prompt us to select a project boilerplate template - let's pick "Hello world". The tool will create a directory called `my-hello-world` with a Node.js project files. We can run the actor as follows:

```bash
cd my-hello-world
apify run
```

## Running Crawlee code as an actor

For running the Crawlee code as an actor on the [Apify platform](https://apify.com/actors) we should either:
- wrap it into [`Actor.main()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#main) function;
- or use a combination of [`Actor.init()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#init) and [`Actor.exit()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#exit) functions.

Let's look at the `CheerioCrawler` example from the [Quick Start](../quick-start) guide:

<Tabs>
    <TabItem value="main" label="Using Actor.main()" default>
        <CodeBlock language="js">
            {MainSource}
        </CodeBlock>
    </TabItem>
    <TabItem value="init_exit" label="Using Actor.init() and Actor.exit()">
        <CodeBlock language="js">
            {InitExitSource}
        </CodeBlock>
    </TabItem>
</Tabs>

Note that we could also run our actor (that is using Crawlee) locally with Apify CLI. We could start it via the following command in our project folder:

```bash
apify run -p
```

## Deploying an actor to Apify platform

Now (assuming we are already logged in to our Apify account) we can easily deploy our code to the Apify platform by running:

```bash
apify push
```

Our script will be uploaded to and built on the Apify platform so that it can be run there. For more information, view the
[Apify Actor](https://docs.apify.com/cli) documentation.

## Usage on Apify platform

We can also develop our actor in an online code editor directly on the platform (we'll need an Apify Account). Let's go to the [Actors](https://console.apify.com/actors) page in the app, click *Create new* and then go to the *Source* tab and start writing our code or paste one of the examples from the [Examples](../examples) section.

## Storages

There are several things worth mentioning here.

1. Compared to Crawlee, in order to simplify access to the default <ApiLink to="core/class/KeyValueStore">`Key-Value Store`</ApiLink> and <ApiLink to="core/class/Dataset">`Dataset`</ApiLink> we don't need to use the helper functions of storage classes, but instead, we could use [`Actor.getValue()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#getValue), [`Actor.setValue()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#setValue) for the default `Key-Value Store` and [`Actor.pushData()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#pushData) for the default `Dataset` directly.
2. In order to open the storage, we shouldn't use the storage classes, but instead use the Actor class. Thus, instead of <ApiLink to="core/class/KeyValueStore#open">`KeyValueStore.open()`</ApiLink>, <ApiLink to="core/class/Dataset#open">`Dataset.open()`</ApiLink> and <ApiLink to="core/class/RequestQueue#open">`RequestQueue.open()`</ApiLink>, we could use [`Actor.openKeyValueStore()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#openKeyValueStore), [`Actor.openDataset()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#openDataset) and [`Actor.openRequestQueue()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#openRequestQueue) respectively. Using each of these methods allows us to pass the [`OpenStorageOptions`](https://apify.github.io/apify-sdk-js/api/apify/interface/OpenStorageOptions) as a second argument, which has only one optional property: [`forceCloud`](https://apify.github.io/apify-sdk-js/api/apify/interface/OpenStorageOptions#forceCloud). If set to `true` - cloud storage will be used instead of the folder on the local disk.
3. When the <ApiLink to="core/class/Dataset">`Dataset`</ApiLink> is stored on the `Apify platform`, we can export its data to the following formats: HTML, JSON, CSV, Excel, XML and RSS. The datasets are displayed on the actor run details page and in the [Storage](https://console.apify.com/storage) section in the Apify Console. The actual data is exported using the [Get dataset items](https://apify.com/docs/api/v2#/reference/datasets/item-collection/get-items) Apify API endpoint. This way we can easily share the crawling results.

**Related links**

- [Apify platform storage documentation](https://docs.apify.com/storage)
- [View storage in Apify Console](https://console.apify.com/storage)
- [Key-value stores API reference](https://apify.com/docs/api/v2#/reference/key-value-stores)
- [Datasets API reference](https://docs.apify.com/api/v2#/reference/datasets)
- [Request queues API reference](https://docs.apify.com/api/v2#/reference/request-queues)

## Environment variables

The following are some additional environment variables specific to Apify platform. More Crawlee specific environment variables could be found in the [Environment Variables](./environment-variables) guide.

:::note

It's important to notice that `CRAWLEE_` environment variables don't need to be replaced with `APIFY_` ones respected by Apify platform. E.g. if we have `CRAWLEE_DEFAULT_DATASET_ID` set in our project, and then we push our code to the Apify platform as an Actor - this variable would still be respected by the Actor/platform.

:::

### `APIFY_TOKEN`

The API token for our Apify account. It is used to access the Apify API, e.g. to access cloud storage
or to run an actor on the Apify platform. We can find our API token on the
[Account - Integrations](https://console.apify.com/account?tab=integrations) page.

### Combinations of `APIFY_TOKEN` and `CRAWLEE_STORAGE_DIR`

> `CRAWLEE_STORAGE_DIR` env variable description could be found in [Environment Variables](../guides/environment-variables#crawlee_storage_dir) guide.

By combining the env vars in various ways, we can greatly influence the actor's behavior.

| Env Vars                                | API | Storages         |
| --------------------------------------- | --- | ---------------- |
|  none OR `CRAWLEE_STORAGE_DIR`          | no  | local            |
| `APIFY_TOKEN`                           | yes | Apify platform   |
| `APIFY_TOKEN` AND `CRAWLEE_STORAGE_DIR` | yes | local + platform |

When using both `APIFY_TOKEN` and `CRAWLEE_STORAGE_DIR`, we can use all the Apify platform
features and our data will be stored locally by default. If we want to access platform storages,
we can use the `{ forceCloud: true }` option in their respective functions.

```js
import { Actor } from 'apify';

const localDataset = await Actor.openDataset('my-local-data');
const remoteDataset = await Actor.openDataset('my-remote-data', { forceCloud: true });
```

### `APIFY_PROXY_PASSWORD`

Optional password to [Apify Proxy](https://docs.apify.com/proxy) for IP address rotation.
Assuming Apify Account was already created, we can find the password on the [Proxy page](https://console.apify.com/proxy)
in the Apify Console. The password is automatically inferred using the `APIFY_TOKEN` env var,
so in most cases, we don't need to touch it. We should use it when, for some reason,
we need access to Apify Proxy, but not access to Apify API, or when we need access to
proxy from a different account than our token represents.

## Proxy management

In addition to our own proxy servers and proxy servers acquired from
third-party providers used together with Crawlee, we can also rely on [Apify Proxy](https://apify.com/proxy)
for our scraping needs.

If we are already subscribed to Apify Proxy, we can start using them immediately in only a few lines of code (for local usage we first we should be [logged in](#logging-into-apify-platform-from-crawlee) to our Apify account.

```javascript
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration();
const proxyUrl = await proxyConfiguration.newUrl();
```

Note that unlike using proxy in Crawlee, we shouldn't use the constructor to create <ApiLink to="core/class/ProxyConfiguration">`ProxyConfiguration`</ApiLink> instance. For using Apify Proxy we should create an instance using the [`Actor.createProxyConfiguration()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#createProxyConfiguration) function instead.

### Apify Proxy vs. Own proxies

The `ProxyConfiguration` class covers both Apify Proxy and custom proxy URLs so that
we can easily switch between proxy providers, however, some features of the class
are available only to Apify Proxy users, mainly because Apify Proxy is what
one would call a super-proxy. It's not a single proxy server, but an API endpoint
that allows connection through millions of different IP addresses. So the class
essentially has two modes: Apify Proxy or Own (third party) proxy.

The difference is easy to remember.
- If we're using our own proxies - we should create an instance with the ProxyConfiguration <ApiLink to="core/class/ProxyConfiguration#constructor">`constructor`</ApiLink> function based on the provided <ApiLink to="core/interface/ProxyConfigurationOptions">`ProxyConfigurationOptions`</ApiLink>.
- If we are planning to use Apify Proxy - we should create an instance using the [`Actor.createProxyConfiguration()`](https://apify.github.io/apify-sdk-js/api/apify/class/Actor#createProxyConfiguration) function - <ApiLink to="core/interface/ProxyConfigurationOptions#proxyUrls">`ProxyConfigurationOptions.proxyUrls`</ApiLink> and <ApiLink to="core/interface/ProxyConfigurationOptions#newUrlFunction">`ProxyConfigurationOptions.newUrlFunction`</ApiLink> enable use of our custom proxy URLs, whereas all the other options are there to configure Apify Proxy.

### Apify Proxy Configuration

With Apify Proxy, we can select specific proxy groups to use, or countries to connect from.
This allows us to get better proxy performance after some initial research.

```javascript
import { Actor } from 'apify';

const proxyConfiguration = await Actor.createProxyConfiguration({
    groups: ['RESIDENTIAL'],
    countryCode: 'US',
});
const proxyUrl = await proxyConfiguration.newUrl();
```

Now our crawlers will use only Residential proxies from the US. Note that we must first get access
to a proxy group before We are able to use it. We can check proxy groups available to us
in the [proxy dashboard](https://console.apify.com/proxy).

**Related links**

- [Apify Proxy docs](https://docs.apify.com/proxy)
